import os
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from tqdm import tqdm
import ollama
from urllib.parse import urlparse, parse_qs, urlunparse, unquote

import subprocess
import sys

def check_ollama_running():
    try:
        result = subprocess.run(
            ["pgrep", "-x", "ollama"], capture_output=True, text=True
        )
        if result.returncode != 0:
            print("‚ö†Ô∏è  Ollama ne semble pas √™tre lanc√©.")
            print("üëâ Lancez-le d'abord avec :  open -a Ollama.app")
            print("   ou d√©marre le service via :  ollama serve &")
            print()
            input("Appuie sur Entr√©e quand Ollama est pr√™t...")
    except Exception as e:
        print(f"Impossible de v√©rifier l‚Äô√©tat d‚ÄôOllama : {e}")
        input("Appuie sur Entr√©e pour continuer...")

check_ollama_running()

# --- Constantes ---
INPUT_DIR = "input"
OUTPUT_HTML = "output.html"
CONCURRENT_REQUESTS = 3
LLM = "gpt-oss:20b"

# --- URLs √† exclure ---
EXCLUDED_PATTERNS = [
    "techcafe.fr",          # exemple : bloque tout ce domaine
    "guillaumevende.fr",
]

# --- Nettoyage d'une URL ---
def clean_url(url: str) -> str:
    """Nettoie les URLs Google et supprime les param√®tres de tracking."""
    url = unquote(url.strip())

    # Liens Google de redirection
    if "google.com/url" in url:
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if "q" in params and params["q"]:
                url = params["q"][0]  # vraie URL contenue dans ?q=
        except Exception:
            pass

    # Supprime les param√®tres inutiles
    parsed = urlparse(url)
    blocked = {
        "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
        "fbclid", "gclid", "mc_cid", "mc_eid", "ref", "sa", "source", "ust", "usg"
    }
    qs = parse_qs(parsed.query)
    cleaned_qs = {k: v for k, v in qs.items() if k not in blocked}
    clean_query = "&".join(f"{k}={v[0]}" for k, v in cleaned_qs.items())
    cleaned = parsed._replace(query=clean_query)
    final = urlunparse(cleaned)

    # Corrige les doubles sch√©mas
    if final.startswith("https://https://") or final.startswith("http://https://"):
        final = final.replace("http://https://", "https://").replace("https://https://", "https://")

    return final


# --- Extraction des URLs ---
def extract_urls_from_html(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")

    urls = [clean_url(a["href"]) for a in soup.find_all("a", href=True)]
    return urls


# --- Sauvegarde des URLs dans un fichier texte ---
def save_urls_to_file(urls, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")


# --- R√©sum√© avec Ollama ---
async def summarize_with_ai(title, content):
    """Utilise le mod√®le local pour r√©sumer un texte."""
    try:
        prompt = (
            f"√âcris un r√©sum√© en fran√ßais le contenu suivant. Attention, sois vigilant au fait de ne faire qu'une seule phrase et ne pas √™tre trop long. Le format est tr√®s important.\n\n"
            f"Ne fais pas tes r√©sum√©s en commen√ßant par 'cet article traite de...' ou √©quivalent. Le but est d'avoir un r√©sum√© qui adresse directement le contenu.\n\n"
            f"Titre : {title}\n\n"
            f"{content}"
        )

        response = ollama.chat(
            model=LLM,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Tu es un assistant qui r√©sume fid√®lement le contenu d'un article et tu t'exprimes toujours en fran√ßais, m√™me si le texte d'origine est en anglais. Tu respectes scrupuleusement les consignes et tu veilles √† ne pas trop r√©diger car tu sais √™tre concis."
                    )
                },
                {"role": "user", "content": prompt}
            ]
        )
        return response["message"]["content"].strip()
    except Exception as e:
        return f"R√©sum√© indisponible ({e})"


# --- T√©l√©chargement du contenu de chaque article ---
async def fetch_article(session, url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }

    try:
        async with session.get(url, headers=headers, ssl=False, timeout=20) as resp:
            if resp.status == 401 or resp.status == 403:
                # Fallback rapide pour sites restreints : log et retourne None
                print(f"üîí {url} ‚Üí HTTP {resp.status} (acc√®s restreint)")
                return None
            if resp.status != 200:
                print(f"‚ùå {url} ‚Üí HTTP {resp.status}")
                return None
            return await resp.text()
    except Exception as e:
        print(f"‚ö†Ô∏è {url} ‚Üí {e}")
        return None

# --- Traitement complet d'une URL ---
async def process_url(session, url, semaphore):
    """
    T√©l√©charge l'article et g√©n√®re un r√©sum√©.
    Retourne toujours (title, summary), m√™me en cas d'erreur.
    """

    async with semaphore:
        try:
            html = await fetch_article(session, url)

            # Si rien n'a √©t√© r√©cup√©r√©
            if not html:
                return "Erreur de chargement", "R√©sum√© indisponible (contenu inaccessible)"

            # Extrait le titre
            soup = BeautifulSoup(html, "html.parser")
            title_tag = soup.find("title")
            title = title_tag.text.strip() if title_tag else "Titre non trouv√©"

            # R√©cup√®re le texte
            # --- Extraction et nettoyage des paragraphes pertinents ---
            paragraphs = []
            for p in soup.find_all("p"):
                t = p.get_text(strip=True)
                if not t:
                    continue

                # Exclusion de textes parasites g√©n√©riques
                if any(x in t.lower() for x in [
                    "cookie", "newsletter", "subscribe", "sign up", "digest",
                    "privacy policy", "advertisement", "adblock", "accept cookies",
                    "breaking bad", "comment", "related article", "get the verge",
                    "connect with", "daily digest", "follow", "open in app"
                ]):
                    continue

                paragraphs.append(t)

            # --- Nettoyage sp√©cifique selon le domaine ---
            domain = urlparse(url).netloc.lower()

            if "techcrunch.com" in domain:
                paragraphs = [p for p in paragraphs if not any(
                    x in p.lower()
                    for x in [
                        "latest news", "artificial intelligence", "amazon apps",
                        "biotech", "climate", "cryptocurrency", "enterprise",
                        "apps", "health", "tc+", "privacy", "advertising"
                    ]
                )]

            elif "theverge.com" in domain:
                paragraphs = [p for p in paragraphs if not any(
                    x in p.lower()
                    for x in [
                        "digest quotidien", "flux d‚Äôaccueil", "daily digest",
                        "articles de ce sujet", "articles de cet auteur",
                        "s‚Äôabonner", "newsletter", "inscrivez-vous", "follow us"
                    ]
                )]

            text = " ".join(paragraphs[:10])

            # Nettoyage sp√©cifique selon le domaine
            domain = urlparse(url).netloc.lower()

            # --- TechCrunch : supprimer les phrases de navigation
            if "techcrunch.com" in domain:
                paragraphs = [p for p in paragraphs if not any(
                    x in p.lower()
                    for x in [
                        "latest news", "artificial intelligence", "amazon apps",
                        "biotech", "climate", "cryptocurrency", "enterprise",
                        "apps", "health", "tc+", "privacy", "advertising"
                    ]
                )]
                text = " ".join(paragraphs[:10])

            # --- The Verge : supprimer les bas de page g√©n√©riques
            elif "theverge.com" in domain:
                paragraphs = [p for p in paragraphs if not any(
                    x in p.lower()
                    for x in [
                        "daily digest", "subscribe", "newsletter", "follow us",
                        "sign up", "cookie", "privacy policy", "advertisement"
                    ]
                )]
                text = " ".join(paragraphs[:10])

            # Si texte vide
            if not text:
                return title, "R√©sum√© indisponible (aucun texte d√©tect√©)"

            # Appel au mod√®le local pour r√©sum√©
            try:
                response = ollama.chat(
                    model=LLM,
                    messages=[{"role": "user", "content": f"R√©sume ce texte en une phrase en fran√ßais : {text}"}],
                )
                summary = response["message"]["content"].strip()
            except Exception as e:
                summary = f"R√©sum√© indisponible ({e})"

            return title, summary

        except Exception as e:
            return "Erreur de chargement", f"R√©sum√© indisponible ({e})"

# --- G√©n√©ration du fichier HTML de sortie ---
from datetime import datetime

async def generate_output_html(urls, output_file):
    """
    G√©n√®re un fichier HTML unique compilant les r√©sum√©s de toutes les URLs.
    Ajoute un en-t√™te avec titre et date, et √©crit les r√©sultats au fur et √† mesure.
    """

    import aiohttp
    import asyncio

    total = len(urls)
    success_count = 0
    error_count = 0

    # Cr√©ation de l'en-t√™te HTML
    header = f"""<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>R√©sum√©s d‚Äôarticles - {datetime.now().strftime("%d/%m/%Y")}</title>
  <style>
    body {{
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
      margin: 40px auto;
      max-width: 900px;
      line-height: 1.6;
      color: #212529;
    }}
    h1 {{
      text-align: center;
      color: #004085;
      border-bottom: 2px solid #004085;
      padding-bottom: 10px;
    }}
    .meta {{
      text-align: center;
      font-size: 0.9em;
      color: #666;
      margin-bottom: 40px;
    }}
    .article {{
      margin-bottom: 20px;
      padding: 12px;
      border-left: 4px solid #004085;
      background: #f8f9fa;
      border-radius: 6px;
    }}
    .error {{
      border-left-color: #d9534f;
      background: #fdf2f2;
    }}
    a {{
      font-weight: bold;
      color: #004085;
      text-decoration: none;
    }}
    a:hover {{
      text-decoration: underline;
    }}
  </style>
</head>
<body>
  <h1>R√©sum√©s d‚Äôarticles</h1>
  <div class="meta">G√©n√©r√© le {datetime.now().strftime("%d/%m/%Y √† %H:%M")}</div>
"""
    footer = "\n</body>\n</html>"

    # √âcrit l'en-t√™te dans le fichier
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(header)

    semaphore = asyncio.Semaphore(5)  # limite de connexions simultan√©es

    async with aiohttp.ClientSession() as session:
        for i, url in enumerate(urls, start=1):
            print(f"üîÑ ({i}/{total}) Traitement de : {url}")
            title, summary = await process_url(session, url, semaphore)

            with open(output_file, "a", encoding="utf-8") as f:
                if "Erreur" in title or "R√©sum√© indisponible" in summary:
                    error_count += 1
                    f.write(f'''
<div class="article error">
  <a href="{url}">‚ö†Ô∏è Erreur de chargement</a><br />
  <span>{summary}</span>
</div>
''')
                else:
                    success_count += 1
                    f.write(f'''
<div class="article">
  <a href="{url}">{title}</a><br />
  <span>{summary}</span>
</div>
''')

    # Ajoute le pied de page
    with open(output_file, "a", encoding="utf-8") as f:
        f.write(f"""
<hr />
<p><strong>Synth√®se du traitement</strong><br />
‚úÖ Articles trait√©s avec succ√®s : {success_count}<br />
‚ö†Ô∏è Erreurs de chargement ou r√©sum√© : {error_count}<br />
üìÑ Total d‚Äôarticles analys√©s : {total}
</p>
{footer}
""")

    print("\n--- Synth√®se du traitement ---")
    print(f"‚úÖ Articles trait√©s avec succ√®s : {success_count}")
    print(f"‚ö†Ô∏è Erreurs de chargement ou r√©sum√© : {error_count}")
    print(f"üìÑ Total d‚Äôarticles analys√©s : {total}")
    print(f"üóÇÔ∏è  Fichier g√©n√©r√© : {output_file}")


def collect_urls_from_directory(input_dir):
    """Parcourt tous les fichiers HTML du dossier input et extrait toutes les URLs."""
    all_urls = []

    for filename in os.listdir(input_dir):
        if not filename.lower().endswith(".html"):
            continue

        path = os.path.join(input_dir, filename)
        urls = extract_urls_from_html(path)
        all_urls.extend(urls)
        print(f"‚úÖ {len(urls)} URL(s) extraites depuis {filename}")

    # Supprime les doublons
    all_urls = list(dict.fromkeys(all_urls))
    print(f"\nüåê Total unique : {len(all_urls)} URL(s) collect√©es dans '{input_dir}'\n")

    return all_urls

# --- Programme principal ---
async def main():
    # R√©cup√®re toutes les URLs de tous les fichiers du dossier input/
    urls = collect_urls_from_directory(INPUT_DIR)

    if not urls:
        print("Aucune URL trouv√©e. V√©rifie que le dossier 'input/' contient des fichiers HTML.")
        return

    # Applique le filtre d‚Äôexclusion s‚Äôil existe
    urls = [u for u in urls if not any(p in u for p in EXCLUDED_PATTERNS)]

    print(f"{len(urls)} URL(s) √† traiter apr√®s filtrage.\n")

    await generate_output_html(urls, OUTPUT_HTML)


if __name__ == "__main__":
    asyncio.run(main())